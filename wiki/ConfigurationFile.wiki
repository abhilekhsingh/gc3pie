= Configuration File = 

<wiki:toc max_depth="3" />

= Introduction =

The [GC3Utils] commands read two configuration files:
  * system-wide one at `/etc/gc3/gc3utils.conf`, and
  * a user-private one from `~/.gc3/gc3utils.conf`.

Both files use the same format. The system-wide one is read first, so
that users can override configuration in their private file.
Configuration data from corresponding sections in the two
configuration files is merged; the value in the user-private file
overrides the one from the system-wide configuration.

The  gc3utils configuration file follows the format understood
by Python `ConfigParser` objects, which is very close to the syntax
used in MS-Windows `.INI` files.
See http://docs.python.org/library/configparser.html 
for reference.

If you try to start any GC3Utils command without having a
configuration file, a sample one (with all resources commented out)
will be copied to the user-private location `~/.gc3/gc3utils.conf` and
an error message will be displayed, directing users to edit the sample
file.

The GC3Libs configuration file consists of several configuration
blocks.  Each configuration block (section) starts with a keyword in
square brackets and contains the configuration options for a specific
part.

The following sections are used by the GC3Libs/GC3Utils programs:
{{{
    [DEFAULT]
    [authorization/<authorization_name>]
    [resource/<resource_name>]
}}}
Sections with a different name can be present but will be ignored.


= 3 Types of Sections =

There are 3 types of sections. 

  * `[DEFAULT]` is for global settings.
  * `[authorization]` is for settings related to identity/authentication (identifying yourself to clusters & grids).
  * `[resource]` is for settings related to a specific computing resource (cluster, grid, etc.)


== DEFAULT Section ==

The `[DEFAULT]` section is optional, but there should be no more than 1.  Its values can be used to insert values in other sections, using the `%(name)s` syntax.  See below for an example of this.

See documentation of the `SafeConfigParser` object at: 
  http://docs.python.org/library/configparser.html


== Authorization Sections ==

There can be more than one `[authorization]` section.  

Each authorization section must begin with a line of the form:
    [authorization/<authorization_name>]

You can have as many `[authorization]` sections as you want, and can have any name.
This allows you to define different authorization methods for different resources.
Each `[resource/<name>]` section will reference one (and one only) authorization
section.


===  Types of Authorization === 

Synopsis:
  * type: one of `voms-proxy`, `grid-proxy`,  or `ssh`
  * usercert: `slcs` or `manual`

{{{ type }}} defines the authorization type that wil be used to access a resource. Currently, there are three supported authorization types:
  * `ssh`: use for resources that will be accessed by opening an SSH connection to the front-end node
  * `voms-proxy`: uses `voms-proxy-init`: use for resources that require a VOMS-enabled Grid proxy.
  * `grid-proxy`: uses `grid-proxy-init`: use for resources that require a Grid proxy (but no VOMS extensions)

{{{ usercert }}} defines how the user certificate is handled.
Currently there are two supported usercert types:
  * `slcs`: user certificate is generated through `slcs-init` request. When `slcs` is selected, `slcs-init` is used to renew expired user certificate.
  * `manual`: user certificate is generated/renewed though an external process and has to be performed by the user outside of the scope of gc3pie. In this case, if user certificate is expired, gc3pie will fail the related authorization module.

For `ssh` type authorization, the following keys must be provided:
  * `type`: must be `ssh`
  * `username`: must be the username to log in as on the remote machine
Any other key/value pair will be ignored.

For `voms-proxy` type auth, the following keys must be provided:
  * `type`: must be `voms-proxy`
  * `vo`: the VO to authenticate with (passed directly to `voms-proxy-init` as argument to the `--vo` command-line switch)
Any other key/value pair will be ignored.

For `grid-proxy` type auth, the following keys must be provided:
  * `type`: must be `grid-proxy`
Any other key/value pair will be ignored.

For `slcs` usercert auth, the  following keys must be provided:
  * `aai_username`: passed directly to `slcs-init` as argument to the `--user` command-line switch
  * `idp`: passed directly to `slcs-init` as argument to the `--idp` command-line switch

For `manual` usercert auth, no additional keys are required.


Allowed combination:

1. voms + slcs 
{{{
 type: voms-proxy
 usercert: slcs
}}}
2. voms + manually generated usercert
{{{
 type: voms-proxy
 usercert: manual
}}}
3. grid-proxy + slcs
{{{
 type: grid-proxy
 usercert: slcs
}}}
4. grid-proxy + manually generated usercert
{{{
 type: grid-proxy
 usercert: manual
}}}
5. ssh connection
{{{
 type: ssh
}}}


Examples: 
{{{
[authorization/smscg]
type = voms-proxy
usercert = slcs
aai_username= <aai_user_name>
idp= uzh.ch
vo = smscg

[authorization/campus]
type = grid-proxy
usercert = manual

[authorization/ssh1]
type = ssh
username = murri # your username here

[authorization/ssh2] # I use a different account name on some resources
type = ssh
username = rmurri 
}}}


== Resource Sections ==

Each resource section must begin with a line of the form:
    [resource/<resource_name>]

You can have as many `resource/name` sections as you want; this
allows you to define many different resources.  Each `resource/***`
section must reference one (and one only) `authorization/name`
section (in the `authorization_type` key).

Resources currently come in two flavours, distinguished by the value
of the `type` key:
  * If `type` is `arc`, then the resource is accessed using the ARC grid middleware;
  * If `type` is `ssh_sge`, then the resource is an SGE batch system, to be accessed by an SSH connection to its front-end node.
#
Every `resource/***` section must reference a valid
`authorization/***` section. Resources of `arc` type can only
reference `voms` type auth sections; resources of `ssh_sge` type can
only reference `ssh` type sections.

Some keys are commmon to all resource types:
  *  authorization_type : must reference the name of a valid `authorization/***` section; only the name (after the `/` must be specified)
  *  max_cores_per_job : Maximum number of CPU cores that a job can request; a resource will be dropped during the brokering process if a job requests more cores than this
  *  max_memory_per_core : Max amount of memory (expressed in GBs) that a job can request
  *  max_walltime : Maximum job running time (in hours)
  *  name : Resource name
  *  ncores : Total number of cores provided by the resource
  *  type : Resource type, one of `arc` or `sge_ssh`
  *  walltime : Maximum job running time (in hours)

=== `arc` resources ===
The `arc_ldap` key should be defined to the LDAP URL of an ARC GIIS
(if no `frontend` is defined) or GRIS (if a `frontend` is given).
If, in addition, the `frontend` key is also defined, then only queues 
belonging to the specified frontend will be considered for brokering.

=== `ssh_sge` resources ===
The `frontend` key should contain the FQDN of the SGE front-end node.
An SSH connection will be attempted to this node, in order to submit
jobs and retrieve status info. 

Since the installation path to supported applications is not known (there
is no information system, differently from ARC), then the path must be
specified here:
  * `gamess_location`: UNIX path name of the directory containing a valid `qgms` script

When a job has finished, the SGE batch system does not (by default)
immediately write its information into the accounting database.  This
creates a time window during which no information is reported about
the job by SGE, as if it never existed.  In order not to mistake this
for a "job lost" error, GC3Libs allow a "grace time": `qacct` job
information lookups are allowed to fail for a certain time span after
the first time `qstat` failed. The duration of this time span is set
with the `sge_accounting_delay` parameter, whose default is 15 seconds
(matches the default in SGE, as of release 6.2): 
  * `sge_accounting_delay`: Time (in seconds) a failure in `qacct` will not be considered critical


= enabling/disabling selected resources =

Any resource can be disabled by adding a line `enabled = false` to its
configuration stanza.  Conversely, a line `enabled = true` will undo
the effect of an `enabled = false` line (possibly found in a different
configuration file).

This way, resources can be temporarily disabled (e.g., the cluster is
down for maintenance) without having to remove them from the
configuration file.

Users can selectively disable or enable resources that are defined in
the system-wide configuration file.  Two main use cases are supported:
the system-wide configuration file `/etc/gc3/gc3utils.conf` lists and
enables all available resources, and users can turn them off in their
private configuration file `~/.gc3/gc3utils.conf`; or the system-wide
configuration can list all available resources but keep them disabled,
and users can enable those they prefer in the private configuration
file.


= Example configuration file =
{{{
[resource/gc3]
# A single cluster, accessed through the ARC middleware
type = arc
authorization_type = <auth_name> # pick a `voms` type auth
frontend = idgc3grid01.uzh.ch
name = gc3
arc_ldap = ldap://idgc3grid01.uzh.ch:2135/mds-vo-name=resource,o=grid
max_cores_per_job = 32
max_memory_per_core = 2
max_walltime = 12
ncores = 80
walltime = 12

[resource/ocikbpra]
# A single SGE cluster, accessed by SSH'ing to the front-end node
type = ssh_sge
authorization_type = <auth_name> # pick an `ssh` type auth, e.g., "ssh1"
frontend = ocikbpra.uzh.ch
gamess_location = /share/apps/gamess
max_cores_per_job = 80
max_memory_per_core = 2
max_walltime = 2
ncores = 80
walltime = 2
}}}