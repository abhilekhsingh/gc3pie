= The `gc3utils` software =

<wiki:toc max_depth="2" />


The `gc3utils` software is a command line front-end to manage
computational job submission to different batch processing systems.
Indeed, `gc3utils` commands can submit [http://www.msg.chem.iastate.edu/gamess/documentation.html GAMESS] 
jobs to the [http://www.smscg.ch SMSCG Grid] or to any computational cluster you
can ssh into (however, currently only the SGE batch systems manager is
supported).

In addition to the command-line interface, `gc3utils` also offer a
Python library for more flexible job submission and control, but that
is an advanced topic and will not be covered here.

== Glossary ==

  * _Core_: A single computing unit; this was called a CPU until manufacturers started packing many processing units into a single package: now the term CPU is used for the package, and "core" is one of the several independent processing units within the package.
  * _Job_: A computational job is a single run of a non-interactive application.  The prototypical example is a run of [http://www.msg.chem.iastate.edu/gamess/documentation.html GAMESS] on a single input file.
  * _Resource_: Short for "computational resource": any cluster or Grid where a job can run.
  * _Status_: A one-word indication of a computational job execution status (e.g., `RUNNING` or `TERMINATED`).
  * _Walltime_: Short for "wall-clock time": indicates the total running time of a job.
  


== The supported computational job workflow ==

A computational job (_job_, for short) is a single run of a
non-interactive application.  The prototypical example is a run of
[http://www.msg.chem.iastate.edu/gamess/documentation.html GAMESS]
on a single input file.

The `gc3utils` commands support the following workflow:

  # Submit a [http://www.msg.chem.iastate.edu/gamess/documentation.html GAMESS] job (with a single input file): _*gsub*_
  # Monitor the status of the submitted job: _*gstat*_
  # Retrieve the output of a job once it's finished: _*gget*_
  
Usage and some examples on how to use the mentioned commands are provided in the next sections.

Some additional commands are provided to obtain the list and status of
computational resources (_*glist*_); to clear the list of jobs from
old and failed ones (_*gclean*_); to get detailed information on a
submitted job (_*ginfo*_, mainly for debugging purposes).


= _*gsub*_: submit a GAMESS job =

To submit a [http://www.msg.chem.iastate.edu/gamess/documentation.html GAMESS] 
job with input file _INPUT.inp_, just type into a terminal window:
{{{
  gsub gamess INPUT.inp
}}}
The `gc3utils` will automatically choose an execution cluster for you,
based on which of the available compute resources has the most free slots.

In case of _successful_ submission, this message will be printed
on the screen:
{{{
  Successfully submitted job.42; use the 'gstat' command to monitor its progress.
}}}
(Of course the job number will change!)  The _*gsub*_ command will
print a different message if something went wrong.

Immediately after running the _*gsub*_ command, you should be able
to see a new job in the _*gstat*_ output.

If you want to specify which
[http://en.wikipedia.org/wiki/Batch_system batch system] should run your
computations, you can use the `-r` option followed by the resource
name like this:
{{{
  gsub -r nh64 gamess INPUT.inp
}}}
You can list resource names with the command _*glist*_ (described below).

The _*gsub*_ command will also pick up a default job size (in terms of
CPU cores, allocated memory, and maximum run-time) for your
[http://www.msg.chem.iastate.edu/gamess/documentation.html GAMESS]
job.  Currently, the default job size is: 2 CPU cores, 1GB of memory
per core, and 1 hour of run-time.

One size does _not_ fit all, so you might want to specify how many
CPU cores and memory your job needs with the `-c` and the `-m`
options, followed (respectively) by the number of CPU cores and
GigaBytes of memory (per core) requested.  For instance, the following command:
{{{
  gsub -c 8 -m 2 gamess INPUT.inp
}}}
will run [http://www.msg.chem.iastate.edu/gamess/documentation.html GAMESS]
on the input file _INPUT.inp_ with 8 CPU cores and
allocate 16GB of memory (2GB per CPU core).

Similarly, you can specify an upper bound to the running time of a job
with the `-w` option followed by the number of hours that the job
should be allowed to run (maximum).  For instance:
{{{
  gsub -w 8 gamess INPUT.inp
}}}
will run [http://www.msg.chem.iastate.edu/gamess/documentation.html GAMESS]
on file _INPUT.inp_ for (at most) 8 hours.

All the above options can be combined and given in any order in a
single command line.  For example, all the following commands are
equivalent; they do the same thing, i.e., run
[http://www.msg.chem.iastate.edu/gamess/documentation.html GAMESS] on
file _INPUT.inp_ on 8 CPU cores, 16 GB of memory, and for a maximum
run-time of 10 hours:
{{{
  gsub -c 8 -m 2 -w 10 gamess INPUT.inp
  gsub -m 2 -c 8 -w 10 gamess INPUT.inp
  gsub -w 10 gamess INPUT.inp -c 8 -m 2 
  gsub gamess INPUT.inp -w 10 -m 2 -c 8 
}}}

If you need to pass additional input files to GAMESS (e.g., an
external basis set), just list them on the command-line *after the
`.inp` file:*
{{{
  gsub gamess INPUT.inp basis.dat
}}}


= _*gstat*_: monitor the status of submitted jobs =

To see the status of all the jobs you have submitted, use the
_*gstat*_ command.  Typing:
{{{
  gstat
}}}
will print to the screen a table like the following:
{{{
Job ID            Status    
===========================
job.12            TERMINATED
job.15            SUBMITTED
job.16            RUNNING
job.17            RUNNING
job.23            NEW
}}}
A job can be in one and only one of the following states:
  * `NEW`: The job has been created but not yet submitted: it only exists on the local disk.  You will never see this status when submitting jobs with the _*gsub*_ command: any job is immediately submitted. However, other commands (e.g., GGamess, GRosetta) employ a two-step submission process.
  * `RUNNING`: The job is currently running -- there's nothing to do but wait.
  * `SUBMITTED`: The job has been sent to a compute resource for execution -- it should change to `RUNNING` status eventually.
  * `STOPPED`: The job was sent to a remote cluster for execution, but it is stuck there for some unknown reason.  There is no automated procedure in this case: the best thing you can do is to contact the systems administrator to determine what has happened.
  * `TERMINATED`: The job has finished running; now there are three things you can do:
     1. Use the _*gget*_ command to get the command output files back from the remote execution cluster.
     2. Use the _*gclean*_ command to remove this job from the list.  After issuing _*gclean*_ on a job, any information on it is lost, so be sure you have retrieved any interesting output with _*gget*_ before!
  * `3. If something went wrong during the execution of the job (it did not complete its execution or -possibly- it did not even start), you can use the _*ginfo*_ command to try to debug the problem.
  
The list of submitted jobs persists from one session to the other: you
can log off, shut your computer down, then turn it on again next day
and you will see the same list of jobs.

*Nota Bene 1:* If you have never submitted any job, or if you have
cleared your job list with the _*gclean*_ command, then _*gstat*_
will print _nothing_ to the screen!

*Nota Bene 2:* Completed jobs persist in the _*gstat*_ list until
they are cleared off with the _*gclean*_ command.


= _*gtail*_: peeking at the job output and error report =

Once a job has reached `RUNNING` status (check with _*gstat*_), you
can also monitor its progress by looking at the last lines in the job
output and error stream.  

An example might clarify this: assume you have submitted a
long-running computation as _job.16_ and you know from _*gstat*_ that
it got into `RUNNING` state; then to take a peek at what this job is
doing, you issue the following command:
{{{
  gtail job.16
}}}
This would produce the following output, from which you can deduce how
far GAMESS has progressed into the computation:
{{{
 RECOMMEND NRAD ABOVE  50 FOR ZETA'S ABOVE 1E+4

 RECOMMEND NRAD ABOVE  75 FOR ZETA'S ABOVE 1E+5

 RECOMMEND NRAD ABOVE 125 FOR ZETA'S ABOVE 1E+6

 DFT IS SWITCHED OFF, PERFORMING PURE SCF UNTIL SWOFF THRESHOLD IS REACHED.


 ITER EX DEM     TOTAL ENERGY        E CHANGE  DENSITY CHANGE    DIIS ERROR

   1  0  0    -1079.0196780290 -1079.0196780290   0.343816910   1.529879639

          * * *   INITIATING DIIS PROCEDURE   * * *

   2  1  0    -1081.1910665431    -2.1713885141   0.056618918   0.105322104

   3  2  0    -1081.2658345285    -0.0747679855   0.019565324   0.044813607
}}}

By default, _*gtail*_ only outputs the last 10 lines of a job
output/error stream. To see more, use the command line option `-n`;
for example, to see the last 25 lines of the output, issue the command: 
{{{
  gtail -n 25 job.16
}}}

The command _*gtail*_ is especially useful for long computations: you
can see how far a job has gotten and, e.g., cancel it if it's gotten
stuck into an endless/improductive loop.

To "keep an eye" over what a job is doing, you can add the `-f` option
to _*gtail*_: this will run _*gtail*_ in "follow" mode, i.e.,
_*gtail*_ will continue to display the contents of the job output and
update it as time passes, until you hit Ctrl+C to interrupt it.


= _*gkill*_: cancel a running job =

To cancel a running job, you can use the command _*gkill*_.  For
instance, to cancel _job.16_, you would type the following command
into the terminal:
{{{
  gkill job.16
}}}

_Nota Bene: There's no undo to a cancel operation!_ Once you have
issued a _*gkill*_ command, the job is deleted and it cannot be
resumed.


= _*gget*_: retrieve the output of finished jobs =

Once a job has reached `RUNNING` status (check with _*gstat*_),
you can retrieve its output files with the _*gget*_ command.  For
instance, to download the output files of _job.15_ you would use:
{{{
  gget job.15
}}}
This command will print out a message like:
{{{
  Job results successfully retrieved in '/path/to/some/directory'
}}}

On your computer, you can copy+paste the path within quotes to the
"sftp" command to get the files to your usual workstation.  For
example, you can run the following command in a terminal on your
computer to get the output files back to your workstation:
{{{
  sftp ocikbgtw:'/path/to/some/directory'
}}}
This will take you to the directory where the output files have been stored.


= _*gclean*_: remove a completed job from the status list =

Jobs persist in the _*gstat*_ list until they are cleared off; you
need to use the _*gclean*_ command for that.

Just call the _*gclean*_ command followed by the job identifier
_job.NNN_.  For example:
{{{
  gclean job.23
}}}

In normal operation, you can only remove jobs that are in the
`COMPLETED`, `FAILED` or `DELETED` status; if you want to force
_*gclean*_ to remove a job that is not in any one of those states,
just add `-f` to the command line.


= _*gresub*_: re-submit a failed job =

In case a job failed for accidental causes (e.g., the site where it
was running went unexpectedly down), you can re-submit it with the
_*gresub*_ command.

Just call _*gresub*_  followed by the job identifier
_job.NNN_.  For example:
{{{
  gresub job.42
}}}

Resubmitting a job that is not in a terminal state (i.e.,
`TERMINATED`) results in the job being killed (as with _*gkill*_)
before being submitted again.  If you are unsure what state
a job is in, check it with _*gstat*_.


= _*glist*_: list available resources =

The _*glist*_ command prints out information about the configured resources.
For each resource, a summary of the information recorded in the configuration
file and the current resource status is printed.  For example:
{{{
$ glist
==============================================================================
Resource: idgc3grid01
Frontend name: idgc3grid01.uzh.ch
Resource access type: arc
Authorization type: smscg
User can access: True
Total number of cores: 320
Queued jobs: 26
Running jobs: 2
Max cores per job: 256
Max memory per core: 2000 (MB)
Max walltime per job: 5999940 (minutes)
[...]
}}}
The meaning of the printed fields is as follows:
  * _Resource:_ this is the "resource name", as you would write it after the `-r` option to _*gsub*_.
  * _Resource access type:_ this is the kind of software that is used for accessing the resource: it currently can be one of `arc` (meaning this is a [http://smscg.ch Grid resource]) or `sge_ssh` (meaning this is a local cluster that you can SSH into).
  * _Authorization type:_ this is paired with the "resource access type", and identifies a section in the [ConfigFile configuration file] where authentication information for this resource is stored; see [ConfigurationFile] for more information.
  * _User can access:_ whether you are _currently_ authorized to access this resource; note that if this turns _False_ for resources that you should have access to, then something is wrong either with the state of your system, or with the resource itself.  (The procedure on how to diagnose this is too complex to list here; consult your friendly systems administrator :-))
  * _Total nunber of cores:_ the total number of cores present on the resource.  Note this can vary over time as cluster nodes go in and out of service: computers break, then are repaired, then break again, etc.
  * _Queued jobs:_ number of jobs waiting to be executed on the remote compute cluster.
  * _Running jobs:_ number of jobs currently executing on the remote compute cluster.
  * _Max cores per job:_ the maximum nunber of cores that you can request for a single computational job on this resource.
  * _Max memory per core:_ maximum amount of memory (per core) that you can request on this resource.
  * _Max walltime per job:_ maximum duration of a computational job on this resource.

The whole point of GC3Utils is to abstract job submission and
management from detailed knowledge of the resources and their hardware
and software configuration, but it is sometimes convenient and
sometimes necessary to get into this level of detail...


= _*ginfo*_: accessing low-level details of a job =

It is sometimes necessary, for debugging purposes, to print out all
the details about a job; the _*ginfo*_ command does just that: prints
all the details that GC3Utils know about a single job.

For instance, to print out detailed information about `job.13` you would type: 
{{{
  ginfo job.13
}}}

For a running job, only basic information is known: where the job is
running, and when it was started:
{{{
==============================================================================
arc_cluster           idgc3grid01.uzh.ch 
arc_cpu_count         1          
arc_job_name          Application 
arc_queue             all.q      
arc_requested_cpu_time  43200      
arc_requested_wall_time  43200      
arc_sstderr           output.txt 
arc_sstdout           output.txt 
arc_submission_time   2011-02-01 17:03:47 
arc_used_cpu_time     4500       
arc_used_memory       2251292    
cores                 1          
lrms_jobid            gsiftp://idgc3grid01.uzh.ch:2811/jobs/83461296576227615555054 
lrms_jobname          Application 
queue                 all.q      
resource_name         idgc3grid01 
stderr_filename       output.txt 
stdout_filename       output.txt 
timestamp             {'RUNNING': 1296576338.814153, 'SUBMITTED': 1296576230.639291} 
used_cputime          4500       
used_memory           2251292    
==============================================================================
}}}
As you can see, most of the output is only useful if you are familiar
with GC3Utils inner working.  Nonetheless, _*ginfo*_ output is definitely
something you should include in any report about a misbehaving job!

For a finished job, the information is more complete and can include
error messages in case the job has failed:
{{{
==============================================================================
arc_cluster           nordugrid.unibe.ch 
arc_completion_time   2011-02-01 17:37:41 
arc_cpu_count         1          
arc_job_name          Application 
arc_queue             all.q      
arc_requested_cpu_time  43200      
arc_requested_wall_time  43200      
arc_sstderr           output.txt 
arc_sstdout           output.txt 
arc_submission_time   2011-02-01 17:04:13 
arc_used_cpu_time     1380       
arc_used_memory       2049966    
arc_used_wall_time    1380       
cores                 1          
error_log             LRMS error: (271) job killed: vmem 
lrms_jobid            gsiftp://nordugrid.unibe.ch:2811/jobs/3004112965762531174281403 
lrms_jobname          Application 
queue                 all.q      
resource_name         ubelix     
stderr_filename       output.txt 
stdout_filename       output.txt 
timestamp             {'TERMINATED': 1296580845.312623, 'RUNNING': 1296577049.303503, 'SUBMITTED': 1296576260.873594} 
used_cputime          1380       
used_memory           2049966    
used_walltime         1380       
==============================================================================
}}}

With option `-v`, _*ginfo*_ output is even more verbose and complete,
and includes a log of all the events that happened during the job's
"lifetime":
{{{
$ ginfo -v job.1356
==============================================================================
_exitcode             255        
_signal               124        
_state                TERMINATED 
arc_cluster           nordugrid.unibe.ch 
arc_completion_time   2011-02-01 17:37:41 
arc_cpu_count         1          
arc_job_name          Application 
arc_queue             all.q      
arc_queue_rank        -1         
arc_requested_cpu_time  43200      
arc_requested_wall_time  43200      
arc_sstderr           output.txt 
arc_sstdin                       
arc_sstdout           output.txt 
arc_submission_time   2011-02-01 17:04:13 
arc_used_cpu_time     1380       
arc_used_memory       2049966    
arc_used_wall_time    1380       
cores                 1          
error_log             LRMS error: (271) job killed: vmem 
exit_code             -1         
log                   SUBMITTED on Tue Feb  1 17:04:20 2011
Submitted to 'ubelix' at Tue Feb  1 17:04:20 2011
RUNNING on Tue Feb  1 17:17:29 2011
Running at Tue Feb  1 17:17:29 2011
ARC reported error: LRMS error: (271) job killed: vmem
TERMINATED on Tue Feb  1 18:20:45 2011
Abnormal termination: 124 
lrms_jobid            gsiftp://nordugrid.unibe.ch:2811/jobs/3004112965762531174281403 
lrms_jobname          Application 
queue                 all.q      
resource_name         ubelix     
stderr_filename       output.txt 
stdout_filename       output.txt 
timestamp             {'TERMINATED': 1296580845.312623, 'RUNNING': 1296577049.303503, 'SUBMITTED': 1296576260.873594} 
used_cputime          1380       
used_memory           2049966    
used_walltime         1380       
==============================================================================
}}}